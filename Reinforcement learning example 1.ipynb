{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 482kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/9d/8296d8211318d690119eba6d293b7a149c1c51c945342dd4c3816f79e1ba/scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
      "\u001b[K     |████████████████████████████████| 27.4MB 563kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /home/dell/snap/jupyter/common/lib/python3.7/site-packages (from gym) (1.20.1)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0 (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow<=7.2.0 (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Collecting future (from pyglet<=1.5.0,>=1.4.0->gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dell/snap/jupyter/6/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dell/snap/jupyter/6/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built gym future\n",
      "Installing collected packages: scipy, future, pyglet, Pillow, cloudpickle, gym\n",
      "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 pyglet-1.5.0 scipy-1.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the  required libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment\n",
    "env=gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Q_table\n",
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "q_table=np.zeros((state_space_size,action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Initialization\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An average reward per thousand episode\n",
      "1000 -- 0.5100000000000003\n",
      "2000 -- 0.6860000000000005\n",
      "3000 -- 0.6800000000000005\n",
      "4000 -- 0.6830000000000005\n",
      "5000 -- 0.7150000000000005\n",
      "6000 -- 0.7190000000000005\n",
      "7000 -- 0.6770000000000005\n",
      "8000 -- 0.6650000000000005\n",
      "9000 -- 0.6830000000000005\n",
      "10000 -- 0.7090000000000005\n",
      "The updated version of Q-table\n",
      "[[0.5619417  0.52074333 0.52308623 0.5266887 ]\n",
      " [0.33702785 0.345486   0.32235424 0.50877577]\n",
      " [0.42581477 0.40307279 0.4242737  0.47861976]\n",
      " [0.32750619 0.2512367  0.32178408 0.46027995]\n",
      " [0.58042353 0.46241195 0.3940604  0.39466907]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18944174 0.15338907 0.4132777  0.10963841]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40728548 0.4653679  0.48897181 0.634066  ]\n",
      " [0.31968037 0.6763655  0.53148344 0.34865316]\n",
      " [0.64674029 0.44003105 0.42749247 0.26132381]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.43523113 0.53154118 0.82908745 0.46308076]\n",
      " [0.76834746 0.94217041 0.78213097 0.73974572]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Q-learning algorithm\n",
    "rewards_all_episodes=[]\n",
    "for episode in range(num_episodes):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    \n",
    "    for steps in range(max_steps_per_episode):\n",
    "        # Exploration and exploitaion trade-off\n",
    "        exploration_rate_threshold=random.uniform(0,1)\n",
    "        if exploration_rate_threshold>exploration_rate:\n",
    "            action=np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info=env.step(action)\n",
    "        #Update Q-value \n",
    "        q_table[state,action]=q_table[state,action]*(1-learning_rate)+learning_rate*(reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state=new_state\n",
    "        rewards_current_episode+= reward\n",
    "        \n",
    "        if done==True:\n",
    "            break\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "# The calcualtion of rewards per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count=1000\n",
    "print(\"An average reward per thousand episode\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    \n",
    "    print(count, \"--\",(sum(r/1000)))\n",
    "    count=count+1000\n",
    "print(\"The updated version of Q-table\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
